<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>– cluster</title><link>https://docs.trustgrid.io/tags/cluster/</link><description>Recent content in cluster on</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Wed, 28 Dec 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://docs.trustgrid.io/tags/cluster/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Clusters</title><link>https://docs.trustgrid.io/docs/clusters/</link><pubDate>Wed, 28 Dec 2022 00:00:00 +0000</pubDate><guid>https://docs.trustgrid.io/docs/clusters/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;p>A cluster is a pair of &lt;a href="https://docs.trustgrid.io/docs/nodes/">nodes&lt;/a> that share configuration and an active/standby relationship, providing automated high-availability (HA) connectivity.&lt;/p>
&lt;/div>
&lt;p>A cluster is a pair of &lt;a href="https://docs.trustgrid.io/docs/nodes/">nodes&lt;/a> at a single site that share some configurations and provide automatic failover. An additional IP address is assigned as a Cluster Virtual IP address that can move between the &lt;a href="https://docs.trustgrid.io/docs/nodes/">nodes&lt;/a> if failover occurs.&lt;/p>
&lt;p>Certain settings such as network services and VPN settings can be configured for the cluster and these settings will override the individual &lt;a href="https://docs.trustgrid.io/docs/nodes/">node&amp;rsquo;s&lt;/a> configuration.&lt;/p>
&lt;div class="alert alert-primary" role="alert">
Formerly the active member was referred to as the “master.” We are in the process moving to the terms active and standby. This documentation will use those terms but elements in the UI may retain the &lt;em>master&lt;/em> term.
&lt;/div>
&lt;h2 id="requirements">Requirements&lt;/h2>
&lt;ul>
&lt;li>Nodes in the cluster must be using the same model of appliances&lt;/li>
&lt;li>Nodes must be able to create direct TCP connections with each other&lt;/li>
&lt;li>Nodes should be at the same site or have high-bandwidth, high-reliability, low-latency connections between the devices&lt;/li>
&lt;/ul>
&lt;h2 id="active-member-determination">Active Member Determination&lt;/h2>
&lt;p>The active member of a cluster is determined by the following factors:&lt;/p>
&lt;ul>
&lt;li>Cluster heartbeat communication&lt;/li>
&lt;li>Cluster mode&lt;/li>
&lt;li>Configured master&lt;/li>
&lt;li>Cluster member health&lt;/li>
&lt;/ul>
&lt;h3 id="cluster-heartbeat-communication">Cluster Heartbeat Communication&lt;/h3>
&lt;p>Cluster members utilize a direct TCP connection to each other to determine if their partner is online and share their health status. Each node will listen on a configured heartbeat IP and port, while at the same time connecting to their partner’s configured heartbeat IP and port.&lt;/p>
&lt;div class="alert alert-primary" role="alert">
Local ACLs allow only the partner cluster member to connect to the listening port.
&lt;/div>
&lt;p>Heartbeat communication is configured on each node&amp;rsquo;s &lt;a href="https://docs.trustgrid.io/docs/nodes/appliances/cluster/">cluster&lt;/a> page.&lt;/p>
&lt;h3 id="cluster-mode">Cluster Mode&lt;/h3>
&lt;p>A cluster can be configured in two different modes to determine what happens when a failed member returns to healthy status:&lt;/p>
&lt;ul>
&lt;li>Automatic Failback (Default) - In this mode the member set as the Configured Master (see below) will maintain the active/master role as long as it is online and healthy.&lt;/li>
&lt;li>Manual Failback - In this mode, the active/master role only moves if either the current holder fails or the configured master is changed&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="cluster-mode.png" alt="img">&lt;/p>
&lt;p>Consider a cluster with members named Node1, the configured master, and Node2.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Event&lt;/th>
&lt;th>Automatic Failback - Active Member&lt;/th>
&lt;th>Manual Failback - Active Member&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Initial State&lt;/td>
&lt;td>Node1&lt;/td>
&lt;td>Node1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Node1 unhealthy/offline&lt;/td>
&lt;td>Node2&lt;/td>
&lt;td>Node2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Node1 returns to healthy/online&lt;/td>
&lt;td>Node1&lt;/td>
&lt;td>Node2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="configured-master">Configured Master&lt;/h3>
&lt;p>Each cluster will have one configured or preferred active member. This is reflected in the overview section.&lt;/p>
&lt;p>&lt;img src="nodes-list.png" alt="img">&lt;/p>
&lt;p>A node may be designated as the preferred active member by selecting the node and clicking the &amp;ldquo;Set as Master&amp;rdquo; action&lt;/p>
&lt;p>&lt;img src="set-as-master.png" alt="img">&lt;/p>
&lt;p>The &lt;code>Configured Master&lt;/code> field will change immediately, but the &lt;code>Current Master&lt;/code> may take a minute to reflect the change as the nodes process the change and notify the control plane.&lt;/p>
&lt;h2 id="cluster-member-health">Cluster Member Health&lt;/h2>
&lt;p>There may be situations where both cluster members are online and can communicate with each other, but external conditions exist that make a node unsuitable to hold the active role. The Trustgrid node service monitors for such conditions and will make a node as unhealthy if one occurs. The node will release the active role and its standby member will take over if it is online and healthy.&lt;/p>
&lt;p>When the condition clears the node will declare itself healthy and inform its partner member. Depending on the cluster mode it may reclaim the active role.&lt;/p>
&lt;h3 id="cluster-member-health-conditions">Cluster Member Health Conditions&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>Loss of &lt;a href="#cluster-heartbeat-communication">cluster heartbeat&lt;/a> communication - If a node cannot communicate with its partner nodes on the configured IP and port it will declare that partner node unhealthy and claim the active role if it has not already.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Interface Link (Up/Down) State - Any interface configured with an IP address in the Trustgrid is monitored for a successful connection to another network device&lt;/p>
&lt;ul>
&lt;li>Example: In a two-interface setup it would be possible for the LAN interface to become unplugged but the node to remain online from the cloud and data plane perspective. However, the lack of LAN connectivity would prevent the node from delivering any Trustgrid services to that network&lt;/li>
&lt;li>Note: If only a single node has an interface configured, such as an alternate MPLS path, Trustgrid support can configure it to be ignored for triggering.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Upstream Internet Issues - If a Trustgrid node is unable to build connections to both the Trustgrid control plane AND data plane connections to its gateways the node will be marked as unhealthy. This does require all the connections to be failing before it is triggered&lt;/p>
&lt;ul>
&lt;li>Example: If an upstream internet provider or device experiences failure the node will not be able to provide any services.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>WAN Interface DHCP failure - If the WAN interface is configured to use DHCP and it does not receive a DHCP lease it will mark itself unhealthy.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Layer 4 (L4 Proxy) Service Health Check - TCP L4 Proxy Services can be configured to regularly perform health checks to confirm a successful connection can be made. If these checks fail 5 times in a row the service will mark the cluster member as unhealthy.&lt;/p>
&lt;ul>
&lt;li>Example: If each cluster member’s LAN interface is connected to a different switch and one switch fails that member will be unable to connect to any IP:ports dependent on that path.&lt;br>
&lt;div class="alert alert-warning" role="alert">
Use caution before configuring multiple services with health checks. This could create a situation where both nodes declare themselves unhealthy because a single service is failing if the server providing the service goes offline.
&lt;/div>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="cluster-member-shared-configuration">Cluster Member Shared Configuration&lt;/h2>
&lt;p>Cluster members can share the configuration for the following services:&lt;/p>
&lt;ul>
&lt;li>Networking
&lt;ul>
&lt;li>Interfaces
&lt;ul>
&lt;li>Interface routes&lt;/li>
&lt;li>Cluster VIP (supported in on-premise, traditional network environment)&lt;/li>
&lt;li>AWS/Azure/GCP route table entries&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>VPN&lt;/li>
&lt;li>Tunnels&lt;/li>
&lt;li>VRFs&lt;/li>
&lt;li>ZTNA gateway endpoints&lt;/li>
&lt;li>Layer 4 services and connectors&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Compute
&lt;ul>
&lt;li>Container and command definitions&lt;/li>
&lt;li>Resource limits&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="tags">Tags&lt;/h2>
&lt;p>&lt;a href="https://docs.trustgrid.io/docs/nodes/shared/tags/">Tags&lt;/a> are visible at the bottom of the overview page for the resource.&lt;/p>
&lt;p>&lt;img src="cluster-tags.png" alt="img">&lt;/p>
&lt;h3 id="modifying-tags">Modifying Tags&lt;/h3>
&lt;p>To add a tag:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Click &lt;code>Add Tag&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A new row will appear at the bottom of the tags table. There will be a list of existing tag names for your organization. You can filter the list by typing in the field. You can either select an existing tag name, or create a new one by typing it out in full and then selecting &lt;code>New selection: tagName&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="new-tag-name.png" alt="img">&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Next move to the values field. As with the name, existing values will be listed. To enter a new value type it in completely.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click &lt;code>Save&lt;/code>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Tag rows can be edited in-place. Change the name or value, then click &lt;code>Save&lt;/code>.&lt;/p>
&lt;p>To remove a tag, click the red X next to the tag name, then click &lt;code>Save&lt;/code>.&lt;/p>
&lt;h3 id="applying-a-tag-filter-to-the-clusters-table">Applying a Tag Filter to the Clusters Table&lt;/h3>
&lt;p>The clusters table can also be filtered to only show clusters with a specific tag name:value.&lt;/p>
&lt;ol>
&lt;li>On the clusters table click &lt;code>Actions&lt;/code> and select &lt;code>Add Tag Filter&lt;/code> from the drop-down menu.&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="add-tag-filter-2.png" alt="img">&lt;/p>
&lt;ol>
&lt;li>After clicking &lt;code>Add Tag Filter&lt;/code>, select the tag-name field and you will see a list of tag-names available. Select the desired tag.&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="pick-tag-filter-name2.png" alt="img">&lt;/p>
&lt;p>You can also start typing to filter what tag names are shown.&lt;/p>
&lt;ol start="3">
&lt;li>
&lt;p>Select the tag value field and you will see a list of available values. Select the desired value.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>(Optional) Click &lt;code>Add Tag Filter&lt;/code> to include an additional filter. Note that the two filters will be applied using AND only clusters with both tag name:value combinations matching will be shown.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Click &lt;code>Apply Tag Filter&lt;/code> and the table will only show matching clusters.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="applied-filters2.png" alt="img">&lt;/p></description></item><item><title>Tutorials: Cluster Failover Response</title><link>https://docs.trustgrid.io/tutorials/operations-runbook/cluster-failover-response/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://docs.trustgrid.io/tutorials/operations-runbook/cluster-failover-response/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;p>When the active (master) member of the &lt;a href="https://docs.trustgrid.io/docs/clusters/">cluster&lt;/a> goes unhealthy the standby member will take over the active role. This process should be automatic and not require manual intervention. However, in certain circumstances, such as the unexpected failover of a public gateway, it is worth investigating to confirm traffic is in a healthy state.&lt;/p>
&lt;/div>
&lt;h3 id="possible-messages">Possible Messages&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>Master role assumed - failover&lt;/p>
&lt;ul>
&lt;li>Indicates the master role has moved from the designated master (primary) to the backup/secondary &lt;a href="https://docs.trustgrid.io/docs/nodes/">node&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Master role reclaimed by expected master&lt;/p>
&lt;ul>
&lt;li>Indicates the master role has returned to the designated master&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="failover-process">Failover Process&lt;/h3>
&lt;p>Below is a brief description events that occur during a failover process:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The &lt;a href="https://docs.trustgrid.io/docs/nodes/">node&lt;/a> assuming the master role will ARP to the network that it now owns the &lt;a href="https://docs.trustgrid.io/docs/clusters/">Cluster&lt;/a> Virtual IP (VIP)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The &lt;a href="https://docs.trustgrid.io/docs/domain/virtual-networks/routes/">Domain route&lt;/a> table will update that the assuming &lt;a href="https://docs.trustgrid.io/docs/nodes/">node&lt;/a> should receive all traffic for the &lt;a href="https://docs.trustgrid.io/docs/clusters/">cluster&lt;/a> (&lt;em>clustername-master&lt;/em>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The assuming &lt;a href="https://docs.trustgrid.io/docs/nodes/">node&lt;/a> will load all NAT entries associated with the &lt;a href="https://docs.trustgrid.io/docs/clusters/">cluster&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="response-process">Response Process&lt;/h3>
&lt;p>After a failover or failback it is necessary to verify that traffic is flowing appropriately.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Login to the portal and navigate to the affected &lt;a href="https://docs.trustgrid.io/docs/clusters/">cluster’s&lt;/a> page&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Verify that only a single &lt;a href="https://docs.trustgrid.io/docs/nodes/">node&lt;/a> shows as master&lt;/p>
&lt;/li>
&lt;li>
&lt;p>On the &lt;code>Configuration&lt;/code> → &lt;code>Network&lt;/code> tab note the &lt;a href="https://docs.trustgrid.io/docs/clusters/">cluster&lt;/a> VIP&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="cluster-virtual-ip2.png" alt="img">&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Click on the indicated current master&lt;/p>
&lt;p>a. Verify the VPN Route Table shows&lt;/p>
&lt;pre>&lt;code>i. Navigate to the `Configuration` → `VPN` tab
ii. Launch the &amp;quot;View Virtual Route Table&amp;quot; tool
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="virtual-network-tools.png" alt="img">&lt;/p>
&lt;pre>&lt;code>iii. Verify that routes show as “available true”
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="routing-tables.png" alt="img">&lt;/p>
&lt;pre>&lt;code> 1. If the cluster is a gateway cluster there may be many routes and not all be active, just confirm many show as available.
2. The route to the management VIP for the other node in the cluster will always be false.
&lt;/code>&lt;/pre>
&lt;p>b. Verify traffic is flowing through the appropriate node.&lt;/p>
&lt;pre>&lt;code>i. Navigate to the `Configuration` → `Network` tab
ii. Confirm the interface associated with the Cluster VIP is selected
3. For single interface nodes: ETH0 / Network Adapter 1 - WAN Adapter
4. For dual interface nodes: ETH1 / Network Adapter 2 - LAN Adapter
iii. Open the `Sniff Interface Traffic` tool.
5. Set the filter to “host clusterVIP” without quotes and replacing clusterVIP with the appropriate Cluster virtual IP. Click `start session`.
6. Confirm that you see traffic flowing through the interface. **Continue monitoring for several minutes to confirm the traffic is maintained.**
7. Leave the Sniff Interface Tool running while completing the next step
iv. Repeat steps i-iii on the node that **is not** currently indicated as the master.
1.You want to verify there is **no traffic for the cluster VIP running through the non-master node**.
8. You may see a periodic ARP from the cluster master, but that should be it.
&lt;/code>&lt;/pre>
&lt;ol>
&lt;li>
&lt;p>Compare traffic volume before and after the failover&lt;/p>
&lt;p>a. If the event was a failover and failback compare traffic from the current master&lt;/p>
&lt;p>b. If the event was a failover but has not failed back you will need to compare traffic volume on the current master to the volume on the previous master&lt;/p>
&lt;pre>&lt;code> i. e.g if traffic failed from node1 to node2, compare node1’s traffic prior to the failover to the volume of traffic on node2 after the failover
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol></description></item></channel></rss>